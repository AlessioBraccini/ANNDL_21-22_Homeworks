{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"The_Neurotic_Networks-HW2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BOnPncRy43Yq"},"outputs":[],"source":["#Imports and preliminary settings\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import random\n","import pandas as pd\n","import seaborn as sns\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import keras.backend as K\n","\n","plt.rc('font', size=16)\n","\n","from sklearn.preprocessing import MinMaxScaler\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","tf.get_logger().setLevel('ERROR')\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","print(tf.__version__)\n","\n","# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","\n","\n"]},{"cell_type":"code","source":["#Calculate some statistics\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive\n","%ls\n","\n","dataset = pd.read_csv(\"Training.csv\")\n","\n","print(\"Shape\", dataset.shape)\n","print(\"\\n\\nMeans\\n\\n\", dataset.mean())\n","\n","print(\"\\n\\nStandard dev\\n\\n\", dataset.std())\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.colheader_justify', 'center')\n","pd.set_option('display.precision', 3)\n","\n","print(\"\\n\\nCovariance matrix\\n\\n\", dataset.cov())\n","\n","print(\"\\n\\nCorrelation matrix\\n\\n\", dataset.corr())"],"metadata":{"id":"rtAvMUaD492e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the dataset\n","\n","# INSPECT DATAFRAME\n","dataset.head()\n","dataset.info()\n","def inspect_dataframe(df, columns):\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n","    for i, col in enumerate(columns):\n","        axs[i].plot(df[col])\n","        axs[i].set_title(col)\n","    plt.show()\n","inspect_dataframe(dataset, dataset.columns)\n","\n","val_size = 3800\n","X_train_raw = dataset.iloc[:-val_size]\n","# y_train_raw = y.iloc[:-val_size]\n","X_val_raw = dataset.iloc[-val_size:]\n","# y_test_raw = y.iloc[-val_size:]\n","print(X_train_raw.shape, X_val_raw.shape)\n","\n","# Normalize both features and labels\n","X_min = X_train_raw.min()\n","X_max = X_train_raw.max()\n","\n","X_train_raw = (X_train_raw-X_min)/(X_max-X_min)\n","X_val_raw = (X_val_raw-X_min)/(X_max-X_min)\n","\n","\n","window = 200\n","stride = 10\n","\n","future = dataset[-window:]\n","future = (future-X_min)/(X_max-X_min)\n","future = np.expand_dims(future, axis=0)\n","print(future.shape)\n","\n","def build_sequences(df, target_labels, window=200, stride=20, telescope=100):\n","    # Sanity check to avoid runtime errors\n","    assert window % stride == 0\n","    dataset = []\n","    labels = []\n","    temp_df = df.copy().values\n","    temp_label = df[target_labels].copy().values\n","    padding_len = len(df)%window\n","\n","    if(padding_len != 0):\n","        # Compute padding length\n","        padding_len = window - len(df)%window\n","        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float64')\n","        temp_df = np.concatenate((padding,df))\n","        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float64')\n","        temp_label = np.concatenate((padding,temp_label))\n","        assert len(temp_df) % window == 0\n","\n","    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n","        dataset.append(temp_df[idx:idx+window])\n","        labels.append(temp_label[idx+window:idx+window+telescope])\n","\n","    dataset = np.array(dataset)\n","    labels = np.array(labels)\n","    return dataset, labels\n","\n","\n","target_labels = dataset.columns\n","\n","# Set this to 864 if use direct forecasting or 1 if use autoregression\n","telescope = 864\n","\n","X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\n","X_test, y_test = build_sequences(X_val_raw, target_labels, window, stride, telescope)\n","print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n","\n","def inspect_multivariate(X, y, columns, telescope, idx=None):\n","    if(idx==None):\n","        idx=np.random.randint(0,len(X))\n","\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n","    for i, col in enumerate(columns):\n","        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n","        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n","        axs[i].set_title(col)\n","        axs[i].set_ylim(0,1)\n","    plt.show()\n","\n","inspect_multivariate(X_train, y_train, target_labels, telescope)\n","\n","input_shape = X_train.shape[1:]\n","output_shape = y_train.shape[1:]\n","batch_size = 64\n","epochs = 200\n","\n"],"metadata":{"id":"BjXQBFpz5aOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This was the best model that we obtained using RNN. With a window=100\n","#it got a test RMSE of 3.67\n","\n","input_shape=X_train.shape[1:]\n","output_shape=(864,7)\n","epochs=200\n","batch_size=64\n","\n","\n","def build_Rnn_model(input_shape, output_shape):\n","    \n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","\n","    #A bunch of convolutions and pooling\n","    conv = tfkl.Conv1D(128,3,padding='same',activation='relu')(input_layer)\n","    pool = tfkl.MaxPool1D()(conv)\n","    \n","    drop = tfkl.Dropout(0.5)(pool)\n","    conv = tfkl.Conv1D(256,3,padding='same',activation='relu')(drop)\n","\n","    pool = tfkl.MaxPool1D()(conv)\n","    drop = tfkl.Dropout(0.5)(pool)\n","\n","    #Bidirectional RNN with 256 units\n","    lstm = tfkl.Bidirectional(tf.keras.layers.SimpleRNN(256,return_sequences=True))(drop)\n","\n","    #One more convolution and pooling\n","    conv = tfkl.Conv1D(256,3,padding='same',activation='relu')(lstm)\n","    pool = tfkl.MaxPool1D()(conv)\n","    \n","    \n","    globavg = tfkl.GlobalAveragePooling1D()(conv)\n","    drop = tfkl.Dropout(0.5)(globavg)\n","    flat = tfkl.Flatten()(drop)\n","\n","    #Fully connected layers\n","    d2 = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu')(flat)\n","    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(d2)\n","\n","    model=tfk.Model(inputs=input_layer, outputs=output_layer, name='best_rnn')\n","\n","    \n","    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(0.00075), metrics=['mae', 'mse'])\n","    \n","    return model\n","\n","\n","mod = build_Rnn_model(input_shape, output_shape)\n","mod.summary()\n","\n","mod.fit(x=X_train, y=y_train, validation_data=(X_test, y_test),epochs=epochs, batch_size=batch_size,callbacks = [\n","        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=15, restore_best_weights=True),\n","        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n","    ])\n","\n"],"metadata":{"id":"JJLFKNoE5vHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This was our best model using custom attention class and \n","#GRU. It achieved a test RMSE of 3.56 with window=150\n","\n","input_shape=X_train.shape[1:]\n","output_shape=(864,7)\n","epochs=200\n","batch_size=64\n","\n","#Our attention class, later used in the network construction\n","class attention(tfkl.Layer):\n","    def __init__(self,**kwargs):\n","        super(attention,self).__init__(**kwargs)\n"," \n","    def build(self,input_shape):\n","        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n","                               initializer='random_normal', trainable=True)\n","        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n","                               initializer='zeros', trainable=True)        \n","        super(attention, self).build(input_shape)\n"," \n","    def call(self,x):\n","        # Alignment scores. Pass them through tanh function\n","        e = K.tanh(K.dot(x,self.W)+self.b)\n","        # Remove dimension of size 1\n","        e = K.squeeze(e, axis=-1)   \n","        # Compute the weights\n","        alpha = K.softmax(e)\n","        # Reshape to tensorFlow format\n","        alpha = K.expand_dims(alpha, axis=-1)\n","        # Compute the context vector\n","        context = x * alpha\n","        context = K.sum(context, axis=1)\n","        return context\n","\n","def create_RNN_with_attention(input_shape, output_shape):\n","    input_layer=tfkl.Input(shape=input_shape)\n","\n","    #A bunch of convolutions and MaxPooling\n","    l = tfkl.Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n","    l = tfkl.MaxPooling1D()(l)\n","    \n","    l = tfkl.Conv1D(filters=128, kernel_size=3, activation='relu')(l)\n","    l = tfkl.MaxPooling1D()(l)\n","    \n","    l = tfkl.Conv1D(filters=256, kernel_size=3, activation='relu')(l)\n","    l = tfkl.MaxPooling1D()(l)\n","    l = tfkl.BatchNormalization()(l)\n","\n","    #Bidirectional GRU\n","    recurrent = tfkl.Bidirectional(tfkl.GRU(512, return_sequences=True, activation='relu'))(l)\n","\n","    #Here we inserted the attention layer\n","    attention_layer = attention()(recurrent)\n","    \n","    #Now two dense layers\n","    drop = tfkl.Dropout(0.4)(attention_layer)\n","    d2=tfkl.Dense(output_shape[-1]*output_shape[-2], trainable=True, activation='relu')(drop)\n","    drop = tfkl.Dropout(0.3)(d2)\n","\n","    #Output\n","    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(d2)\n","\n","    model=tfk.Model(input_layer,output_layer, name=\"best_attention\")\n","\n","    model.compile(loss='mse', optimizer=tfk.optimizers.Adam(0.00075), metrics=['mae'])\n","\n","    return model\n","\n","mod = create_RNN_with_attention(input_shape, output_shape)\n","mod.summary()\n","\n","mod.fit(x=X_train, y=y_train, validation_data=(X_test, y_test),epochs=epochs, batch_size=batch_size,callbacks = [\n","        tfk.callbacks.EarlyStopping(monitor='val_mae', mode='min', patience=25, restore_best_weights=True),\n","        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n","    ])\n"],"metadata":{"id":"XE0mb-I96O4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Finally, our best model. The first quarter was predicted by mod1,\n","#whose code is now reported.\n","\n","input_shape=X_train.shape[1:]\n","output_shape=(288,7) #Output shape is smaller because mod1 only predicts the 1t quarter\n","epochs=200\n","batch_size=64\n","\n","#In order to comply with mod1 predicting only a subset of the future, we \n","#had to modify the build_sequence functionw when training it.\n","#This version is the original one, the one used when we tried to create one\n","#custom model per quarter. This is why it returns labels_q1, labels_q2, labels_q3.\n","def build_sequences(df, target_labels=['pollution'], window=500, stride=20, telescope=864):\n","    # Sanity check to avoid runtime errors\n","    assert window % stride == 0\n","    dataset = []\n","    labels_q1 = []\n","    labels_q2 = []\n","    labels_q3 = []\n","    temp_df = df.copy().values\n","    temp_label = df[target_labels].copy().values\n","    padding_len = len(df)%window\n","\n","    if(padding_len != 0):\n","        # Compute padding length\n","        padding_len = window - len(df)%window\n","        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float64')\n","        temp_df = np.concatenate((padding,df))\n","        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float64')\n","        temp_label = np.concatenate((padding,temp_label))\n","        assert len(temp_df) % window == 0\n","\n","    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n","        dataset.append(temp_df[idx:idx+window])\n","        labels_q1.append(temp_label[idx+window:idx+window+telescope//3])\n","        labels_q2.append(temp_label[idx+window+telescope//3:idx+window+2*telescope//3])\n","        labels_q3.append(temp_label[idx+window+2*telescope//3:idx+window+telescope])\n","\n","    dataset = np.array(dataset)\n","    labels_q1 = np.array(labels_q1)\n","    labels_q2 = np.array(labels_q2)\n","    labels_q3 = np.array(labels_q3)\n","    return dataset, labels_q1, labels_q2, labels_q3\n","\n","\n","target_labels=dataset.columns\n","val_ratio=0.1\n","\n","telescope = 864\n","\n","X_train, y_train_1, y_train_2, y_train_3 = build_sequences(X_train_raw, target_labels, window, stride, telescope)\n","X_test, y_test_1, y_test_2, y_test_3 = build_sequences(X_val_raw, target_labels, window, stride, telescope)\n","\n","#It is a simple convolutional model using LSTM. At first we tried creating\n","#3 of these models, training each to predict exactly one quarter. This still improved our results\n","#allowing us to achieve a 3.53 RMSE by using a window=100. Then, we tried to only keep the 1st of these models to predicts \n","#the 1st quarter,while the remaining two were predicted by the attention model that is shown above. \n","#This resulted in a 3.4 test RMSE.\n","\n","def build_model1(input_shape, output_shape):\n","    \n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","\n","    conv = tfkl.Conv1D(64, 13, padding='same', activation='relu')(input_layer)\n","    bn = tfkl.BatchNormalization()(conv)\n","\n","    conv = tfkl.Conv1D(128, 3, padding='same', activation='relu')(bn)\n","    bn = tfkl.BatchNormalization()(conv)\n","\n","    mp = tfkl.MaxPool1D()(bn)\n","    bn = tfkl.BatchNormalization()(mp)\n","\n","    lstm = tfkl.Bidirectional(tfkl.LSTM(1024, return_sequences=True))(bn)\n","    bn = tfkl.BatchNormalization()(lstm)\n","\n","    gap = tfkl.GlobalAveragePooling1D()(bn)\n","    dp = tfkl.Dropout(.3)(gap)\n","\n","    dense = tfkl.Dense(output_shape[-1] * output_shape[-2], activation='relu')(dp)\n","    output_layer = tfkl.Reshape((output_shape[-2], output_shape[-1]))(dense)\n","    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same')(output_layer)\n","\n","    # Connect input and output through the Model class\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n","\n","    # Compile the model\n","    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics=['mae'])\n","\n","    # Return the model\n","    return model\n","\n","\n","mod1 = build_model1(input_shape, output_shape)\n","mod1.summary()\n","\n","mod1.fit(x=X_train, y=y_train_1, validation_data=(X_test, y_test_1),epochs=epochs, batch_size=batch_size,callbacks = [\n","        tfk.callbacks.EarlyStopping(monitor='val_mae', mode='min', patience=15, restore_best_weights=True),\n","        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n","    ])\n","\n","\n","#For completeness, we also show the model.py's code for this submission\n","\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","\n","class model:\n","    def __init__(self, path):\n","        #Model to predict the first quarter\n","        self.model1 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel1'))\n","\n","        #Model to predict the second and third quarter\n","        self.model2 = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel'))\n","\n","    def predict(self, X):\n","        \n","        # Insert your preprocessing here\n","        X=X.numpy()\n","        X_min = X.min(axis=0)\n","        X_max = X.max(axis=0)\n","        \n","        future = X[-100:]\n","        future = (future - X_min) / (X_max - X_min)\n","        future = np.expand_dims(future, axis=0)\n","        \n","        future2 = X[-100:]\n","        future2 = (future2 - X_min) / (X_max - X_min)\n","        future2 = np.expand_dims(future2, axis=0)\n","\n","        #Predict the 1st quarter\n","        out1 = self.model1.predict(future)\n","\n","        #Predict the rest\n","        out2 = self.model2.predict(future2)\n","\n","        # Insert your postprocessing here\n","\n","        out1 = out1 * (X_max - X_min) + X_min  # denormalize\n","        #Reshape\n","        out1 = np.reshape(out1, (288, 7))\n","        \n","        out2 = out2 * (X_max - X_min) + X_min  # denormalize\n","        #Reshape\n","        out2 = np.reshape(out2, (288*3, 7))\n","        #out2 = tf.convert_to_tensor(out2)\n","        \n","        #Discard the first quarter predicted by model2, i.e. the first 288 values.\n","        out2 = out2[288:288*3]\n","        \n","        #Concatenate to achieve the full prediction\n","        out = np.concatenate((out1,out2), axis=0)\n","        out = tf.convert_to_tensor(out)\n","\n","        return out"],"metadata":{"id":"L1Xbd2qe8e7w"},"execution_count":null,"outputs":[]}]}